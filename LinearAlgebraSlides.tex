\documentclass[landscape]{slides}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\title{Univariate Normal Distribution}
\author{Zhaoxia Yu}
\date{September 2016}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\maketitle

\begin{slide}{\Large Univariate Normal Distribution}
\begin{itemize}
    \item Notation: $X\sim N(\mu, \sigma^2)$, where $E(X)=\mu, Var(X)=E[(X-\mu)^2]=\sigma^2$. For a normally distributed random variable, its distribution is completely determined by the two parameters.
    \item pdf: $$f(x)=\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}, -\infty <x <\infty$$
    \item The moment generating function (MGF) of
    $M_X(t)=E[e^{tX}]=e^{\mu t + \frac{1}{2} \sigma^2 t^2}$
\end{itemize}
\end{slide}

\begin{slide}{\Large A random sample from a normal distribution}
\begin{itemize}
    \item What is a random sample? It consists of independent and identically distributed (iid) random variables. 
    \item A normal random sample of size:
    $X_1,\cdots,X_n \overset{iid} \sim N(\mu,\sigma^2)$
    \item The joint distribution of $X_,\cdots,X_n$? 
\end{itemize}
\end{slide}

\begin{slide}{\Large A random sample from a normal distribution}
\begin{itemize}
    \item The joint distribution, i.e., the distribution of the random vector of length $n$, is a multivariate normal distribution. 
    \item The joint probability density is
    \begin{align*}
    f(x_1,\cdots,x_n) & \overset{iid} = f(x_1)\cdots f(x_n)\\
    & = \left( \frac{1}{\sqrt{2\pi \sigma^2}}\right)^n e^{-\frac{(x_1-\mu)^2}{2\sigma^2}} \cdots e^{-\frac{(x_n-\mu)^2}{2\sigma^2}}\\
    & = (2\pi \sigma^2)^{-n/2} e^{-\frac{\sum_{i=1}^n(x_i-\mu)^2}{2\sigma^2}}
    \end{align*}
\end{itemize}
\end{slide}


\begin{slide}{\Large Summary Statistics and Their Expectations}
\begin{itemize}
    \item The sample mean $\bar X$ is defined as $\bar X=\frac{1}{n}\sum_{i=1}^n X_i$.
    \item $\bar X$ is unbiased for $\mu$, i.e., $E(\bar X)=\mu$. $Var(\bar X)=\sigma^2/n$.
    \item The sample variance $S^2\overset{def}=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2$. 
    \item $S^2$ is unbiased for $\sigma^2$, i.e., $E(S^2)=\sigma^2$.
\end{itemize}
\end{slide}

\begin{slide}{\Large The Sampling Distributions of the Summary Statistics}
\begin{itemize}
    \item The sample eman follows a normal distribution. It can be proved using MGF. Specifically,
    $$\bar X \sim N(\mu, \sigma^2/n)$$
    Note, any linear combination of a set of independent normal r.v.s follows a normal distribution.
    \item Distribution of the sample variance:
    $$\frac{(n-1)S^2}{\sigma^2}= \frac{\sum_{i=1}^n (X_i-\bar X)^2}{\sigma^2} \sim \chi_{n-1}^2$$
\end{itemize}
\end{slide}

\begin{slide}{\Large Chi-squared Distribution}
\begin{itemize}
    \item If $X_i\sim N(\mu,\sigma^2)$, then $\left(\frac{X_i-\mu}{\sigma}\right)\sim \chi_1^2$
    \item Let $X_1,\cdots,X_n$ be a random sample from $N(\mu,\sigma^2)$, then $$\frac{\sum_{i=1}^n (X_i-\mu)^2}{\sigma^2}\sim \chi_n^2$$
    \item Connections between normal, chi-squared, $t$ and $F$ distributions
\end{itemize}
\end{slide}

\begin{slide}{\Large One-Sample t-test}
\begin{itemize}
    \item Let $X_1,\cdots,X_n\overset{iid}\sim N(\mu,\sigma^2)$.
    \item We are interested a hypothesis, i.e., a statement about the parameters: $H_O: \mu=\mu_0$.
    \item The t-statistic can be used to examine whether we have enough evidence to be against $H_0$:
    $$t=\frac{\bar X - \mu_0}{SE[\bar X]}=\frac{\bar X - \mu_0}{\sqrt{ \hat{Var}(\bar X)}} \overset{H_0}\sim t_{n-1}$$
    where $\hat{Var}(\bar X)=S^2/n$
\end{itemize}
\end{slide}

\begin{slide}{\Large Two-Sample t-test}
\begin{itemize}
    \item Two INDEPENDENT random samples: $X_1,\cdots,X_m\overset{iid}\sim N(\mu_X,\sigma^2)$, $Y_1,\cdots,Y_n\overset{iid}\sim N(\mu_Y,\sigma^2)$. 
    \item We are interested a hypothesis, i.e., a statement about the parameters: $H_O: \mu_X=\mu_Y$.
    \item The t-statistic can be used to examine whether we have enough evidence to be against $H_0$:
    $$t=\frac{\bar X - \mu_Y}{SE[\bar X - \bar X_Y]}=\frac{\bar X - \bar_Y}{\sqrt{ \hat{Var}(\bar X-\bar Y)}}=\frac{\bar X - \bar_Y}{\sqrt{S_X^2/m+S_Y^2/n}}\overset{H_0}\sim t_{m+n-2},$$
    where $S_X^2$ and $X_Y^2$ are the sample variances.
\end{itemize}
\end{slide}

\begin{slide}{\Large Bivariate Normal Distribution}
Suppose we have a random vector $X_i=\begin{pmatrix}X_{i1}\\X_{i2}\end{pmatrix}$ that follows 
$$N_2(\mu=\begin{pmatrix}\mu_1\\ \mu_2\end{pmatrix}, \Sigma=\begin{pmatrix}\sigma_{1}^2 & \sigma_{12}\\
\sigma_{12}& \Simga_{2}^2\end{pmatrix})$$
\begin{itemize}
    \item Parameters in a bivariate normal distribution: $\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, \sigma_{12}$.
    \item Alternative parameterization: $\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, \rho_{12}$
    \item PDF: a little bit complicated. The expression is much easier if we use vector notations:
    $$f(x)=(2\pi)^{-\frac{2}{2}}|\Sigma|^{-\frac{1}{2}}exp\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\}$$
\end{itemize}
\end{slide}

\begin{slide}{\Large Bivariate Normal Distribution}
\begin{itemize}
    \item What is the marginal distribution of $X_{i1}$?
    \item What is the conditional distribution of $X_{i1}$ given $X_{i2}$?
    \item What condition does the parameter need to satisfy such that $X_{i1}$ and $X_{i2}$ are independent?
\end{itemize}
\end{slide}

\begin{slide}{\Large Bivariate Normal Distribution}
\begin{itemize}
    \item A random sample $X_1,\cdots,X_n$ from the bivariate normal $N(\mu,\Sigma)$.
    \item How to estimate $\mu$? A natural choice is the vector of sample means
    \item How to estimate $\Sigma$? The sample variance-covariance. 
    \item What are their sampling distributions? How to make inference about the parameters?
\end{itemize}
\end{slide}

\begin{slide}{\Large A Brief Review of Matrix Algebra}
\begin{itemize}
    \item In a bivariate (more generally, multivariate) case, we have vectors and matrices. It is necessary to have a brief review of matrix algebra.
\end{itemize}
\end{slide}

\begin{slide}{\Large Vectors}
\begin{itemize}
    \item A vector (column vector) is a special matrix consisting of a single column of elements.
    $$x= \begin{pmatrix}a\\b\\c\end{pmatrix} , y=\begin{pmatrix}1.2\\1.3\\0.5\end{pmatrix} $$
    \item A row vector is a special matrix consisting of a single row of elements. The transpose of a column vector is a row vector. Vise versa.
        $$x^T=(a,b,c), y^T=(1.2,1.3,0.5)$$
    \end{itemize}
\end{slide}


\begin{slide}{\Large Linearly Independence}
\begin{itemize}
    \item Def: Vectors $x_1,\cdots,x_k$, where $x_i \in R^n, i=1,\cdots,n$, are called \underline{linearly dependent} if there exist numbers $\lambda_1,\cdots,\lambda_k$, not all zero, such that
    $$\lambda_1x_1+\cdots+\lambda_kx_k=0$$
    In other words, they are linearly dependent if at least one of them can be written as a linear combination of the others.
    \item Def: A set of vectors are called \underline{linearly independent} if they are not linearly dependent.
\end{itemize}
\end{slide}

\begin{slide}{\Large Linearly Independence}
\begin{itemize}
    \item An alternative definition. Vectors $x_1,\cdots,x_k$, where $x_i \in R^n, i=1,\cdots,n$, are said to be \underline{linearly independent} if $\lambda_1x_1+\cdots+\lambda_kx_k=0$ implies $\lambda_1=\cdots=\lambda_k=0$. If $x_1,\cdots,x_k$ are not linearly independent, we say they are linearly dependent.
    \item Example. $x_1=\begin{pmatrix}1 \\ 1 \end{pmatrix}, x_2=\begin{pmatrix}1 \\ 0 \end{pmatrix}$. 
    $\lambda_1x_1+\lambda_2x_2=0$ implies that $\begin{pmatrix}\lambda_1+\lambda_2 \\ \lambda_1\end{pmatrix}=0 \Rightarrow \lambda_1=\lambda_2=0$. Thus, the two vectors are linearly independent.
\end{itemize}
\end{slide}

\begin{slide}{\Large Linearly Independence}
\begin{itemize}
    \item Orthogonality implies linearly independence, but linearly independence does not guarantee orthogonality.
    \item e.g., $(1,0)^T$ and$(0,1)^T$ are orthogonal; they are also linearly independent.
    \item e.g., $(1,1)^T$ and $(1,0)^T$ are linearly independent but not orthogonal. 
    \item Linearly independent and statistically independent are not the same!
\end{itemize}
\end{slide}

\begin{slide}{\Large Vector Space}
\begin{itemize}
    \item A set of vectors which are closed under addition and scalar multiplication is called a \underline{vector space}.
    \item Consider the a set of vectors $x_1,\cdots, x_p$ where $x_i\in R^n, i=1,\cdots,p$. The linear span by the set of vectors is 
    $Span(x_1,\cdots,x_p)=\{y=\sum_{i=1}^p c_p x_p\}$
    where $c_1,\cdots,c_p$ are scalars. 
    \item It is obvious that the column space is a subspace of $n$-dimensional Euclidean space.
    \item Example. $x_1=(1,1,1)^T,x_2=(1,0,0)^T$. Is $(1,1,1)^T$ in the linear span? How about $(0,1,2)^T$?
\end{itemize}
\end{slide}  

\begin{slide}{\Large Orthogonalization}
\begin{itemize}
    \item Given linearly independent vectors $x_1,\cdots, x_p$, there exists a set of rothogonal vectors $u_1,\cdots,u_p$ with the same linea span. 
    \item The Gram-Schmidt Process
    \begin{align*}
        u_1 &=x_1 &\\
        u_2 &= x_2 - Proj_{u_1}(x_2) = x_2 - \frac{u_1^Tx_2}{u_1^Tu_1}u_1&\\
        \vdots & &\vdots\\
        u_p &= x_p - Proj_{u_1}(x_p) - \cdots - Proj_{u_{p-1}}(x_p) &\\
        &= x_p - \frac{u_1^x_p}{u_1^Tu_1}u_1-\cdots-\frac{u_{p-1}^Tx_p}{u_{p-1}^T u_{p-1}}u_{p-1} &
    \end{align*}
\end{itemize}
\end{slide}  

\begin{slide}{\Large Eigen Values}
\begin{itemize}
    \item
\end{itemize}
\end{slide}  

\begin{slide}{\Large }
\begin{itemize}
    \item
\end{itemize}
\end{slide}  



\end{document}
