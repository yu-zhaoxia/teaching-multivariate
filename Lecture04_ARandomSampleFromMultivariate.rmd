---
title: "Multivariate Analysis Lecture 4: A Random Sample from A Multivariate Distribution"
author: | 
  | Zhaoxia Yu
  | Professor, Department of Statistics
date: "`r Sys.Date()`"
output: 
  beamer_presentation:
    theme: "Ilmenau"
    slide_level: 3
    keep_tex: true
header-includes:
- \setbeamertemplate{navigation symbols}{}
- \setbeamertemplate{footline}[page number]
- \usepackage{amsmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyr) #the pipe (%>%) tool is extremely useful
library(MASS)
library(corrplot)#for visualizing the corr matrix of the iris data
library(car)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  paste0("\n \\", "footnotesize","\n\n", x, "\n\n \\Huge")
})
```


# Outline
- Review of Lecture 03
- Inference of Means
- Inference of a Linear Combination of Mean
- A Simulation Study
- Generalized Variance
- Normal and Multivariate Normal

# Lecture 3
## Random Samples
- Univariate distribution: If $X_1, \cdots, X_n\overset{iid}\sim (\mu, \sigma^2)$, then
  - $\bar X\sim (\mu, \sigma^2/n)$
  - $E[s^2]=\sigma^2$ where $s^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2$. 

- Multivariate distribution: If $X_1, \cdots, X_n\overset{iid}\sim (\boldsymbol \mu, \boldsymbol \Sigma)$, then
  - $\bar{X}\sim (\boldsymbol \mu, \frac{1}{n}\boldsymbol \Sigma)$
  - $E[\mathbf S]=\boldsymbol \Sigma$ where $\mathbf S=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})(X_i-\bar{X})^T$.

## Linear Combinations of a Random Vector

### Random Vectors
- Let $\mathbf X_{p\times 1}=(X_1, \cdots, X_p)^T$ be a random vector. Its mean is defined as 
  $$E[\mathbf X]=\begin{pmatrix}
  E[X_1]\\ \vdots \\ E[X_p]
  \end{pmatrix}$$

- Its covariance matrix is defined as 

  $$\boldsymbol \Sigma_{p\times p} = Cov(\mathbf X) = E[(\mathbf X -\boldsymbol \mu)(\mathbf X - \boldsymbol \mu)^T]$$

- If $\mathbf X\sim (\boldsymbol \mu, \boldsymbol \Sigma)$, then
$Y=\mathbf a^T\mathbf X\sim (\mathbf a^T\boldsymbol \mu, \mathbf a^T\boldsymbol \Sigma \mathbf a)$. 

### Linear Combinations of Random Vectors
- What is a linear combination? E.g., $\mathbf X = (X_1, X_2, X_3)^T$, $a=(1/3, 1/3, 1/3)^T$. Then
$$Y=a^TX=\frac{1}{3}(X_1 + X_2 + X_3)$$

- Note, $Y$, $\mathbf a^T \boldsymbol \mu$, and $\mathbf a^T \boldsymbol \Sigma \mathbf a$ are all scalars.

- $\mathbf a^T \boldsymbol \mu$ is the innuer product between $\mathbf a$ and $\boldsymbol \mu$, i.e., $\sum_{i=1}^p a_i \mu_i$. 
- $\mathbf a^T \boldsymbol \Sigma \mathbf a$ is the quadratic form of $\boldsymbol \Sigma$, i.e., $\sum_{i=1}^p\sum_{j=1}^p a_i a_j \sigma_{ij}$, where $\sigma_{ij}$ is the $(i,j)$th element of $\boldsymbol \Sigma$.

### Linear Combinations: Example
- Example 1: Assume we have a random sample from a distribution with mean $\mu$ and variance $\sigma^2$, i.e., $X_1, \cdots, X_n\overset{iid}\sim (\mu, \sigma^2)$.

- We often stack the random variables vertically:
$$\mathbf X_{n\times 1}=\begin{pmatrix}
X_1 \\ \vdots \\ X_n\end{pmatrix}.$$
- An equivalent expression,  $\mathbf X=(X_1, \cdots, X_n)^T$.
- Note that $\mathbf X$ is a \textcolor{red}{random vector} with mean vector and covariance matrix
  $$E[\mathbf X]=\mu \mathbf 1_n =\begin{pmatrix}\mu \\ \vdots \\ \mu\end{pmatrix},
  Cov(\mathbf X)=\sigma^2 \mathbf I_n = \begin{bmatrix} 
\sigma^2 & 0 & \cdots & 0 \\ 
0 & \sigma^2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\ 
0 & 0 & \cdots & \sigma^2 
\end{bmatrix}
$$


### Linear Combinations: Example
- We can express the sample mean as a linear combination of the random vector $\mathbf X$: 
$$\bar X= \frac{1}{n}\mathbf 1^T \mathbf X,$$
where $\mathbf 1=(1, \cdots, 1)^T$ is a $n\times 1$ vector.
- By the linear combination results, we have
\begin{align*}
E[\bar X]&=\frac{1}{n} \mathbf 1^T E[\mathbf X]=\frac{1}{n} \mathbf 1^T \mu \mathbf 1=\mu\\
Var[\bar X]&=\frac{1}{n^2} \mathbf 1^T Cov(\mathbf X) \mathbf 1=\frac{1}{n^2} \mathbf 1^T \sigma^2 \mathbf I_n \mathbf 1 = \frac{1}{n}\sigma^2.
\end{align*}

# Inference of Means
## Univariate
- A random sample $X_1, \cdots, X_n$ from a univariate distribution with mean $\mu$ and variance $\sigma^2$.
- We are interested in making inference about the population mean $\mu$.
- The sample mean $\bar X$ is an unbiased estimator of $\mu$, i.e., $E[\bar X]=\mu$. We often use $\hat\mu=\bar X$ to estimate $\mu$.
- How to quantify the uncertainty of $\hat\mu$? Recall that $Var(\bar X)=\frac{\sigma^2}{n}$..
- $\sigma^2$ is unknown. But the sample variance $s^2$ is an unbiased estimator of $\sigma^2$, i.e., $E[s^2]=\sigma^2$. We often use $\hat\sigma^2=s^2$ to estimate $\sigma^2$.
- The sample variance $s^2$ is defined as
$$s^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2$$

### Standard Error of $\bar X$
- The standard error of $\bar X$ is defined as $se(\bar X)=\sqrt{Var(\bar X)}=\frac{\sigma}{\sqrt{n}}$.
- We can estimate it by $se(\bar X)=\frac{s}{\sqrt{n}}$.
- A ``large-sample'' (approximate) confidence interval for $\mu$ is given by
$$\bar X \pm z_{\alpha/2} se(\bar X)$$
where $z_{\alpha/2}$ is the upper $\alpha/2$ quantile of the standard normal distribution.
- A ``small-sample'' (approximate) confidence interval for $\mu$ is given by
$$\bar X \pm t_{\alpha/2} se(\bar X)$$
where $t_{\alpha/2}$ is the upper $\alpha/2$ quantile of the t-distribution with $n-1$ degrees of freedom.


## Multivariate
### A Random Sample From a Multivariate Distribution
- Consider a random sample $\mathbf X_1, \cdots, \mathbf X_n$ from a multivariate distribution with mean vector $\boldsymbol \mu_{p\times 1}$ and covariance matrix $\boldsymbol \Sigma_{p\times p}$. 
- We often stack the random vectors to form an $n\times p$ matrix: 
$$\mathbf X_{n\times p}=\begin{pmatrix}\mathbf X_1^T \\ \vdots \\ \mathbf X_n^T\end{pmatrix}$$


### A Random Sample From a Multivariate Distribution: Sample Mean Vector
- Sample mean vector is $$\bar{\mathbf X}=\frac{1}{n}\sum_{i=1}^n \mathbf X_i=(\frac{1}{n}\mathbf 1_n^T \mathbf X)^T$$ 
It is a random vector with 
  - mean vector $E[\bar{\mathbf X}]=\boldsymbol \mu$, i.e., the sample mean vector is unbaised for the population mean vector. $\bar{\mathbf X}$ can be used to estimate $\boldsymbol \mu$.
  - covariance matrix $Cov(\bar{\mathbf X}) = \frac{1}{n} \boldsymbol \Sigma$


### A Random Sample From a Multivariate Distribution: Sample Covariance Matrix
- The sample covariance matrix is
$$\mathbf S = \frac{1}{n-1}\sum_{i=1}^n(\mathbf X_i-\bar{\mathbf X})(\mathbf X_i-\bar{\mathbf X})^T$$
- It is unbiased for $\boldsymbol \Sigma$, i.e., $E[\mathbf S]=\boldsymbol \Sigma$. 

- We showed that 
$$\mathbf S= \frac{1}{n-1} \mathbf X^T \mathbf C \mathbf X $$
where $\mathbf C_{n\times n}=\mathbf I-\frac{1}{n}\mathbf J=\mathbf I-\frac{1}{n}\mathbf 1 \mathbf 1^T$
- This expression is helpful when we derive the distribution of $\mathbf S$.


# Inference of a Linear Combination of Means
## Linear Combinations of Means
- In many situations, the parameter of interest is a function of the means.
- For example, we may be interested in the mean of a linear combination of the means, i.e., $\boldsymbol \mu^T \mathbf a$, where $\mathbf a=(a_1, \cdots, a_p)^T$ is a $p\times 1$ vector.
- In the following simulated study, we will show how to construct a large-sample confidence interval for $\boldsymbol \mu^T \mathbf a$.

## A Simulated Study

### Daily Intake of Protein 
- This is a simulated data set
- For adults, the recommended range of daily protein intake is between 0.8 g/kg and 1.8 g/kg of body weight
- 60 observations
- 4 sources of proteins
  - meat
  - dairy
  - vegetables / nuts / tofu
  - other
  
### Choose Mean Vector and Covariance Matrix
- The multivariate distribution has 
  - mean vector 
  $$\boldsymbol \mu=\begin{pmatrix}24, 16, 8, 8\end{pmatrix}^T$$
  - covariane matrix
  $$\boldsymbol \Sigma=4* \begin{pmatrix}
1.3 & 0.3 & 0.3 & 0.3\\
0.3 & 1.3 & 0.3 & 0.3\\
0.3 & 0.3 & 1.3 & 0.3\\
0.3 & 0.3 & 0.3 & 1.3
  \end{pmatrix}$$

### Define Mean Vector and Covariance Matrix in R
```{r}
#the library "MASS" is required
library(MASS)
my.cov=4*(diag(4) + 0.3* rep(1,4)%o%rep(1,4))
eigen(my.cov)#to check whether the cov matrix is p.d.
my.mean=8*c(3,2,1,1)
n=60
```
  
### Simulate A Random Sample
```{r}
set.seed(1)
x=mvrnorm(n, mu=my.mean, Sigma=my.cov)
dim(x)
protein=as.matrix(data.frame(meat=x[,1],dairy=x[,2], 
                             veg=x[,3], other=x[,4]))
```

### The simulated data
```{r}
protein
```

### Sample Mean and Sample Covariance
```{r}
xbar=matrix(colMeans(protein), 4, 1)
t(xbar)
S=cov(protein)
S
```

### Estimation
- An unbiased estimator of $\boldsymbol\mu$ is the sample mean vector, i.e.,  $\hat{\boldsymbol \mu}=\bar{\mathbf X}$.
- An unbiased estimator of $\boldsymbol \Sigma$ is the sample covariance matrix $\mathbf S$, i.e., $\hat{\boldsymbol \Sigma}=\mathbf S$
- We have shown that $Var(\bar{\mathbf X})=\frac{1}{n}\boldsymbol \Sigma$, where $n=60$. 
- We can estimate it by
$$\hat{Var}(\bar{\mathbf X})=\frac{1}{60}\mathbf S$$


### Linear Functions/Combinations: Three Questions
- Suppose we only have a random sample and we would like to make inference of the following:
- Q1: Construct a large-sample (approximate) C.I. for protein from meat. In other words, the parameter of interest is $\mu_1$.

- Q2: Construct a large-sample C.I. for the total protein intake

- Q3: Construct a large-sample C.I. for the difference of protein intake between from meat and from vegetable


### Linear Functions/Combinations: Question 1
- Q1: Construct a large-sample (approximate) C.I. for protein from meat. In other words, the parameter of interest is $\mu_1$.
- Estimate $\bar X_{(1)}=24.0$.
- We need compute the standard error (s.e.) of $\bar X_1$, which is defined as $se(\bar X_{(1)})=\sqrt{\hat{var}(\bar X_{(1)})}$ 
- Two ways to compute the s.e.,
    1. $se(\bar X_{(1)})=\sqrt{4.2956/60}=0.27$
    2. The calculation can also be done by noticing that $\bar X_1$ is a linear combination of $\bar{\mathbf X}$: $\bar X_{(1)} =\mathbf a^T  \bar{\mathbf X}$, where $\mathbf a^T=(1, 0, 0, 0)$. Thus,

$$\hat{Var}(\bar X_{(1)})=\mathbf a^T \frac{\mathbf S}{60} \mathbf a$$



### Linear Functions/Combinations: Question 2
- Q2: Construct a large-sample C.I. for the total protein intake
- The parameter of interest is $\mu_1+\mu_2+\mu_3+\mu_4=\mathbf a^T \boldsymbol \mu$, where $\mathbf a=(1,1,1,1)^T$.
- Estimate: $\mathbf a^T \bar{\mathbf X}$
- Standard error: $\sqrt{\mathbf a^T\frac{\mathbf S}{n} \mathbf a}$


### Linear Functions/Combinations: Question 3
- Q3: Construct a large-sample C.I. for the difference of protein intake between from meat and from vegetable
- The parameter of interest is $\mu_1 - \mu_3=\mathbf a^T \boldsymbol \mu$, where $\mathbf a=(1, 0, -1, 0)^T$.
- Estimate: $\mathbf a^T \bar{\mathbf X}$
- Standard error: $\sqrt{\mathbf a^T\frac{\mathbf S}{n} \mathbf a}$

### Linear Functions/Combinations: Question 1 (continued)
- R code to compute using the above two ways

```{r}
sqrt(S[1,1]/60) # Method 1

# Method 2
a=matrix(c(1,0,0,0),4,1)
sqrt(t(a)%*%S%*%a/60)
```

* Both methods give $s.e.(\bar X_{(1)})=0.27$

* An approximate 95% C.I. for $\mu_1$ is $24.0 \pm 1.96*0.27$

### Linear Functions/Combinations: Question 2 (continued)
```{r}
a=matrix(1,4,1)
t(a)%*% xbar #estimate 
sqrt(t(a)%*%S%*%a/60) #standard error
#a large-sample 95% C.I. 
c(t(a)%*% xbar- 1.96*sqrt(t(a)%*%S%*%a/60), 
  t(a)%*% xbar+ 1.96*sqrt(t(a)%*%S%*%a/60))
```

### Linear Functions/Combinations: Question 3 (continued)
```{r}
a=matrix(c(1,0,-1,0),4,1)
t(a)%*% xbar #estimate 
sqrt(t(a)%*%S%*%a/60) #standard error
#a large-sample 95% C.I. 
c(t(a)%*% xbar- 1.96*sqrt(t(a)%*%S%*%a/60), 
  t(a)%*% xbar+ 1.96*sqrt(t(a)%*%S%*%a/60))
```


# Generalized Variance

### Why Do We Need Generalized Variance?
- For a random variable (i.e., univaraite), we quantity dispersion using variance and standard deviation. 

- For a random vector (i.e., multivariate), we use its covariance matrix to quantify the dispersion as well as the relationships between different variables /features.  
  - The dispersion information is represented by a matrix, which has $p(p+1)/2$ unique parameters


### What is Generalized Variance?
- It is attempting to have a scalar summary (i.e., a single number) to quantify the "total" amount of dispersion for a multivariate distribution

- Generalized variance
  - Provides an overall measure of dispersion of the multivariate distribution
  - One choice is the determinant: $|\boldsymbol \Sigma|$. 
  - A larger determinant indicates a greater degree of dispersion

### Generalized Variance: An Example
$$
\boldsymbol \Sigma_1=\begin{pmatrix}5 &4 \\ 4 &5\end{pmatrix},
\boldsymbol \Sigma_2=\begin{pmatrix}3 &0 \\ 0 &3\end{pmatrix},
\boldsymbol \Sigma_3=\begin{pmatrix}5 &-4 \\ -4 &5\end{pmatrix}
$$
```{r}
Sigma1=matrix(c(5,4,4,5), 2,2) 
X1=mvrnorm(100, mu=c(0,0), Sigma=Sigma1)
Sigma2=matrix(c(3,0,0,3), 2,2)
X2=mvrnorm(100, mu=c(0,0), Sigma=Sigma2)
Sigma3=matrix(c(5,-4,-4,5), 2,2)
X3=mvrnorm(100, mu=c(0,0), Sigma=Sigma3)
```

### Generalized Variance: An Example
```{r, out.width="100%", out.height="50%"}
par(mfrow=c(1,3))
plot(X1); plot(X2); plot(X3)
```

### Generalized Variance might (NOT) be useful
- In the example above, $|\Sigma_1|=|\Sigma_2|=|\Sigma_3|=9$!
- $|\boldsymbol \Sigma|$ does not tell the orientations.
- $|\boldsymbol \Sigma|$ is useful to compare two patterns when they have nearly the same orientations.
- The generalized variance does not capture all the information contained in the covariance matrix.
- The eigenvalues provide more information than the determinant - Principal Component Analysis!



# Normal Distributions: univariate, multivariate, matrix normal distributions

### The Big Picture: Univariate vs Multivariate
- \textcolor{red}{Review}: A random sample, denoted by $X_1, \cdots, X_n$, from a (univariate) normal distribution $N(\mu, \sigma^2)$
  - What are the distributions of $\bar X,  s^2$? What useful statistics can be constructed?

- \textcolor{red}{New material}: A random sample, denoted by $\mathbf X_1, \cdots, \mathbf X_n$, from a multivariate normal distribution $N(\boldsymbol \mu, \boldsymbol \Sigma)$
  - What are the distributions of $\bar{\mathbf X},  \mathbf S$? What useful statistics can be constructed?
  
## Derivation of t-test
- A random sample $X_1, \cdots, X_n$ from a univariate Normal distribution with mean $\mu$ and variance $\sigma^2$.
$$X_1, \cdots, X_n \overset{iid}\sim N(\mu, \sigma)^2$$
- The sample mean $\bar X \sim N(\mu, \sigma^2/n)$. Standardized the sample mean:
$$\frac{\bar X - \mu}{\sigma/\sqrt{n}} \sim N(0, 1)$$
- The sample variance $s^2 \sim \chi^2_{n-1}$, i.e., $\frac{(n-1)s^2}{\sigma^2} \sim \chi^2_{n-1}$.
- The sample variance $s^2$ is independent of $\bar X$.


### Derivation of t-test
- The t-statistic is defined as
$$t=\frac{\bar X - \mu}{s/\sqrt{n}}=\frac{\sqrt{n}(\bar X - \mu)}{s}$$
- It follows a t-distribution with $n-1$ degrees of freedom, denoted by $t_{n-1}$.

- The t-distribution is a family of distributions that are symmetric and bell-shaped, like the standard normal distribution, but have heavier tails.
- The t-distribution is used in hypothesis testing and confidence intervals for small sample sizes, particularly when the population standard deviation is unknown.

- The t-distribution approaches the standard normal distribution as the sample size increases.

### The Big Picture: Univariate
\small
- A random sample, denoted by $X_1, \cdots, X_n$, from a (univariate) normal distribution $N(\mu, \sigma^2)$
- Let $\mathbf X_{n\times 1}=(X_1, \cdots, X_n)^T$. It is random vector with a multivarite normal distribution, i.e., 
  $$\mathbf X_{n\times 1}=(X_1, \cdots, X_n)^T \sim \mathbf N(\mu\mathbf 1, \sigma^2\mathbf I)$$
1. $\bar X \sim N(\mu, \sigma^2/n)$
2. $\frac{(n-1)s^2}{\sigma^2} \sim \chi_{n-1}^2$
3. Independence between $\bar X$ and $s^2$.
4. a t-statistic is 
$$\frac{\frac{\bar X-\mu}{\sqrt{\sigma^2/n}}}{\sqrt{\frac{(n-1)s^2/\sigma^2}{n-1}}}=\frac{\sqrt{n}(\bar X-\mu)}{s}$$
It follows the t-distribution with n-1 degrees of freedom, denoted by $t_{n-1}$.
\normalsize

### The Big Picture: Multivariate  
- A random sample $\mathbf X_1, \cdots, \mathbf X_n$ from a multivariate normal distribution $\mathbf N(\boldsymbol \mu, \boldsymbol \Sigma)$.
- Let $$\mathbf X_{n\times p}=\begin{pmatrix}
\mathbf X_1^T \\ \vdots \\\mathbf X_n^T
\end{pmatrix}$$ 
$\mathbf X$ follows a matrix normal distribution.

1. Sample mean vector follows a multivariate normal, i.e., $\bar{\mathbf X} \sim \mathbf N(\boldsymbol \mu, \boldsymbol \Sigma/n)$

2. Sample covariance matrix $(n-1)\mathbf S$ follows a Wishart distribution, i.e., 
$(n-1)\mathbf S \sim Wishart_p (n-1, \Sigma)$

3. Independence between $\bar {\mathbf X}$ and $S$.
4. Hoetelling's $T^2$: $T^2 = (\bar{\mathbf X} - \boldsymbol \mu)^T\left(\frac{\mathbf S}{n}\right)^{-1} (\bar{\mathbf X} - \boldsymbol \mu)$
